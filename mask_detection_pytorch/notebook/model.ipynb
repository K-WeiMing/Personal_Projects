{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml_proj/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# TypeHinting\n",
    "from typing import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_box(obj)->List[int]:\n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "\n",
    "def generate_label(obj)->int:\n",
    "    if obj.find(\"name\").text == \"with_mask\":\n",
    "        return 1\n",
    "    elif obj.find(\"name\").text == \"mask_weared_incorrect\":\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def generate_target(image_id:int, file:str)->dict:\n",
    "    \"\"\"Generates the targets of the given iamge\n",
    "\n",
    "    Args:\n",
    "        image_id (int): Index of the file\n",
    "        file (str): File path\n",
    "\n",
    "    Returns:\n",
    "        dict: Labelled targets of the input file (*.xml)\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "        \n",
    "        num_objs = len(objects)\n",
    "        \n",
    "        # Get bounding boxes for objects\n",
    "        # Notes:\n",
    "        # CoCo format: bbox-> [xmin, ymin, width, height]\n",
    "        # PyTorch format: bbox-> [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "        \n",
    "        # Converts data into a tensor, sharing data and preserving autograd history if possible.\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([image_id])\n",
    "        \n",
    "        # Annotation in dictionary format\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = img_id\n",
    "        \n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset:\n",
    "    def __init__(self, transforms):\n",
    "        # Load all image files, sorting to ensure they are aligned\n",
    "        \n",
    "        curr_dir = os.getcwd()\n",
    "        self.img_path = os.path.join(curr_dir, \"../data/images/\")\n",
    "        self.label_path = os.path.join(curr_dir, \"../data/annotations/\")\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(self.img_path)))\n",
    "        self.labels = list(sorted(os.listdir(self.label_path)))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Load images and annotation masks\n",
    "        file_image = 'maksssksksss'+ str(index) + '.png'\n",
    "        file_label = 'maksssksksss'+ str(index) + '.xml'\n",
    "        \n",
    "        img_path = os.path.join(self.img_path, file_image)\n",
    "        label_path = os.path.join(self.label_path, file_label)\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # Generate label\n",
    "        target = generate_target(index, label_path)\n",
    "        \n",
    "        # Check if transforms is loaded\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        # Returns the image and labels\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Transformation and `collate_fn()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaskDataset(data_transform)\n",
    "data_loader = DataLoader(dataset, batch_size=4, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes:int):\n",
    "    \n",
    "    # Load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml_proj/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml_proj/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 79., 105., 109., 142.],\n",
      "        [185., 100., 226., 144.],\n",
      "        [325.,  90., 360., 141.]]), 'labels': tensor([0, 1, 0]), 'image_id': tensor([0])}, {'boxes': tensor([[321.,  34., 354.,  69.],\n",
      "        [224.,  38., 261.,  73.],\n",
      "        [299.,  58., 315.,  81.],\n",
      "        [143.,  74., 174., 115.],\n",
      "        [ 74.,  69.,  95.,  99.],\n",
      "        [191.,  67., 221.,  93.],\n",
      "        [ 21.,  73.,  44.,  93.],\n",
      "        [369.,  70., 398.,  99.],\n",
      "        [ 83.,  56., 111.,  89.]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0]), 'image_id': tensor([1])}, {'boxes': tensor([[ 68.,  42., 105.,  69.],\n",
      "        [154.,  47., 178.,  74.],\n",
      "        [238.,  34., 262.,  69.],\n",
      "        [333.,  31., 366.,  65.]]), 'labels': tensor([1, 1, 1, 2]), 'image_id': tensor([2])}, {'boxes': tensor([[ 52.,  53.,  73.,  76.],\n",
      "        [ 72.,  53.,  92.,  75.],\n",
      "        [112.,  51., 120.,  68.],\n",
      "        [155.,  60., 177.,  83.],\n",
      "        [189.,  59., 210.,  80.],\n",
      "        [235.,  57., 257.,  78.],\n",
      "        [289.,  60., 309.,  83.],\n",
      "        [313.,  68., 333.,  90.],\n",
      "        [351.,  35., 364.,  59.]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([3])}]\n"
     ]
    }
   ],
   "source": [
    "for imgs, annotations in data_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k:v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    \n",
    "    # Test / Sanity check for the first iteration\n",
    "    print(annotations)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 10 / 214, losses: 1.6696\n",
      "Epoch: 1, Step: 20 / 214, losses: 0.9823\n",
      "Epoch: 1, Step: 30 / 214, losses: 0.5432\n",
      "Epoch: 1, Step: 40 / 214, losses: 1.8423\n",
      "Epoch: 1, Step: 50 / 214, losses: 0.1456\n",
      "Epoch: 1, Step: 60 / 214, losses: 0.0611\n",
      "Epoch: 1, Step: 70 / 214, losses: 0.4439\n",
      "Epoch: 1, Step: 80 / 214, losses: 0.3607\n",
      "Epoch: 1, Step: 90 / 214, losses: 0.0666\n",
      "Epoch: 1, Step: 100 / 214, losses: 0.4841\n",
      "Epoch: 1, Step: 110 / 214, losses: 0.2758\n",
      "Epoch: 1, Step: 120 / 214, losses: 0.3792\n",
      "Epoch: 1, Step: 130 / 214, losses: 0.7358\n",
      "Epoch: 1, Step: 140 / 214, losses: 0.7931\n",
      "Epoch: 1, Step: 150 / 214, losses: 0.2166\n",
      "Epoch: 1, Step: 160 / 214, losses: 0.3321\n",
      "Epoch: 1, Step: 170 / 214, losses: 0.6620\n",
      "Epoch: 1, Step: 180 / 214, losses: 0.0069\n",
      "Epoch: 1, Step: 190 / 214, losses: 0.5310\n",
      "Epoch: 1, Step: 200 / 214, losses: 0.7132\n",
      "Epoch: 1, Step: 210 / 214, losses: 0.4327\n",
      "Epoch: 1, Step: 214 / 214, losses: 0.3064\n",
      "Epoch: 1 / 10, Epoch loss: 91.98088073730469\n",
      "Epoch: 2, Step: 10 / 214, losses: 0.7383\n",
      "Epoch: 2, Step: 20 / 214, losses: 0.4137\n",
      "Epoch: 2, Step: 30 / 214, losses: 0.4254\n",
      "Epoch: 2, Step: 40 / 214, losses: 0.8254\n",
      "Epoch: 2, Step: 50 / 214, losses: 0.0952\n",
      "Epoch: 2, Step: 60 / 214, losses: 0.0224\n",
      "Epoch: 2, Step: 70 / 214, losses: 0.3134\n",
      "Epoch: 2, Step: 80 / 214, losses: 0.2345\n",
      "Epoch: 2, Step: 90 / 214, losses: 0.0562\n",
      "Epoch: 2, Step: 100 / 214, losses: 0.2626\n",
      "Epoch: 2, Step: 110 / 214, losses: 0.2388\n",
      "Epoch: 2, Step: 120 / 214, losses: 0.3508\n",
      "Epoch: 2, Step: 130 / 214, losses: 0.4167\n",
      "Epoch: 2, Step: 140 / 214, losses: 0.6108\n",
      "Epoch: 2, Step: 150 / 214, losses: 0.0794\n",
      "Epoch: 2, Step: 160 / 214, losses: 0.2608\n",
      "Epoch: 2, Step: 170 / 214, losses: 0.4562\n",
      "Epoch: 2, Step: 180 / 214, losses: 0.0024\n",
      "Epoch: 2, Step: 190 / 214, losses: 0.3271\n",
      "Epoch: 2, Step: 200 / 214, losses: 0.4702\n",
      "Epoch: 2, Step: 210 / 214, losses: 0.4201\n",
      "Epoch: 2, Step: 214 / 214, losses: 0.1624\n",
      "Epoch: 2 / 10, Epoch loss: 59.82600021362305\n",
      "Epoch: 3, Step: 10 / 214, losses: 0.6585\n",
      "Epoch: 3, Step: 20 / 214, losses: 0.4158\n",
      "Epoch: 3, Step: 30 / 214, losses: 0.3128\n",
      "Epoch: 3, Step: 40 / 214, losses: 0.7632\n",
      "Epoch: 3, Step: 50 / 214, losses: 0.0875\n",
      "Epoch: 3, Step: 60 / 214, losses: 0.0149\n",
      "Epoch: 3, Step: 70 / 214, losses: 0.2119\n",
      "Epoch: 3, Step: 80 / 214, losses: 0.2026\n",
      "Epoch: 3, Step: 90 / 214, losses: 0.0753\n",
      "Epoch: 3, Step: 100 / 214, losses: 0.2347\n",
      "Epoch: 3, Step: 110 / 214, losses: 0.1912\n",
      "Epoch: 3, Step: 120 / 214, losses: 0.4640\n",
      "Epoch: 3, Step: 130 / 214, losses: 0.3763\n",
      "Epoch: 3, Step: 140 / 214, losses: 0.5740\n",
      "Epoch: 3, Step: 150 / 214, losses: 0.1126\n",
      "Epoch: 3, Step: 160 / 214, losses: 0.1997\n",
      "Epoch: 3, Step: 170 / 214, losses: 0.4303\n",
      "Epoch: 3, Step: 180 / 214, losses: 0.0035\n",
      "Epoch: 3, Step: 190 / 214, losses: 0.3091\n",
      "Epoch: 3, Step: 200 / 214, losses: 0.4247\n",
      "Epoch: 3, Step: 210 / 214, losses: 0.5344\n",
      "Epoch: 3, Step: 214 / 214, losses: 0.1436\n",
      "Epoch: 3 / 10, Epoch loss: 50.2819938659668\n",
      "Epoch: 4, Step: 10 / 214, losses: 0.5354\n",
      "Epoch: 4, Step: 20 / 214, losses: 0.3334\n",
      "Epoch: 4, Step: 30 / 214, losses: 0.3648\n",
      "Epoch: 4, Step: 40 / 214, losses: 0.7007\n",
      "Epoch: 4, Step: 50 / 214, losses: 0.0455\n",
      "Epoch: 4, Step: 60 / 214, losses: 0.0018\n",
      "Epoch: 4, Step: 70 / 214, losses: 0.1849\n",
      "Epoch: 4, Step: 80 / 214, losses: 0.2094\n",
      "Epoch: 4, Step: 90 / 214, losses: 0.0658\n",
      "Epoch: 4, Step: 100 / 214, losses: 0.1748\n",
      "Epoch: 4, Step: 110 / 214, losses: 0.1633\n",
      "Epoch: 4, Step: 120 / 214, losses: 0.2753\n",
      "Epoch: 4, Step: 130 / 214, losses: 0.2460\n",
      "Epoch: 4, Step: 140 / 214, losses: 0.4664\n",
      "Epoch: 4, Step: 150 / 214, losses: 0.0574\n",
      "Epoch: 4, Step: 160 / 214, losses: 0.1869\n",
      "Epoch: 4, Step: 170 / 214, losses: 0.3480\n",
      "Epoch: 4, Step: 180 / 214, losses: 0.0009\n",
      "Epoch: 4, Step: 190 / 214, losses: 0.2572\n",
      "Epoch: 4, Step: 200 / 214, losses: 0.3717\n",
      "Epoch: 4, Step: 210 / 214, losses: 0.3041\n",
      "Epoch: 4, Step: 214 / 214, losses: 0.0746\n",
      "Epoch: 4 / 10, Epoch loss: 43.808162689208984\n",
      "Epoch: 5, Step: 10 / 214, losses: 0.4019\n",
      "Epoch: 5, Step: 20 / 214, losses: 0.3701\n",
      "Epoch: 5, Step: 30 / 214, losses: 0.2489\n",
      "Epoch: 5, Step: 40 / 214, losses: 0.6494\n",
      "Epoch: 5, Step: 50 / 214, losses: 0.0556\n",
      "Epoch: 5, Step: 60 / 214, losses: 0.0014\n",
      "Epoch: 5, Step: 70 / 214, losses: 0.1376\n",
      "Epoch: 5, Step: 80 / 214, losses: 0.1099\n",
      "Epoch: 5, Step: 90 / 214, losses: 0.0356\n",
      "Epoch: 5, Step: 100 / 214, losses: 0.1355\n",
      "Epoch: 5, Step: 110 / 214, losses: 0.1701\n",
      "Epoch: 5, Step: 120 / 214, losses: 0.3259\n",
      "Epoch: 5, Step: 130 / 214, losses: 0.2176\n",
      "Epoch: 5, Step: 140 / 214, losses: 0.3920\n",
      "Epoch: 5, Step: 150 / 214, losses: 0.0365\n",
      "Epoch: 5, Step: 160 / 214, losses: 0.1496\n",
      "Epoch: 5, Step: 170 / 214, losses: 0.2951\n",
      "Epoch: 5, Step: 180 / 214, losses: 0.0006\n",
      "Epoch: 5, Step: 190 / 214, losses: 0.2576\n",
      "Epoch: 5, Step: 200 / 214, losses: 0.2813\n",
      "Epoch: 5, Step: 210 / 214, losses: 0.3238\n",
      "Epoch: 5, Step: 214 / 214, losses: 0.0670\n",
      "Epoch: 5 / 10, Epoch loss: 38.717464447021484\n",
      "Epoch: 6, Step: 10 / 214, losses: 0.4182\n",
      "Epoch: 6, Step: 20 / 214, losses: 0.3977\n",
      "Epoch: 6, Step: 30 / 214, losses: 0.3901\n",
      "Epoch: 6, Step: 40 / 214, losses: 0.5884\n",
      "Epoch: 6, Step: 50 / 214, losses: 0.0421\n",
      "Epoch: 6, Step: 60 / 214, losses: 0.0036\n",
      "Epoch: 6, Step: 70 / 214, losses: 0.1509\n",
      "Epoch: 6, Step: 80 / 214, losses: 0.1438\n",
      "Epoch: 6, Step: 90 / 214, losses: 0.0356\n",
      "Epoch: 6, Step: 100 / 214, losses: 0.1381\n",
      "Epoch: 6, Step: 110 / 214, losses: 0.0685\n",
      "Epoch: 6, Step: 120 / 214, losses: 0.2507\n",
      "Epoch: 6, Step: 130 / 214, losses: 0.2629\n",
      "Epoch: 6, Step: 140 / 214, losses: 0.5055\n",
      "Epoch: 6, Step: 150 / 214, losses: 0.0666\n",
      "Epoch: 6, Step: 160 / 214, losses: 0.1149\n",
      "Epoch: 6, Step: 170 / 214, losses: 0.2399\n",
      "Epoch: 6, Step: 180 / 214, losses: 0.0007\n",
      "Epoch: 6, Step: 190 / 214, losses: 0.2415\n",
      "Epoch: 6, Step: 200 / 214, losses: 0.3056\n",
      "Epoch: 6, Step: 210 / 214, losses: 0.2814\n",
      "Epoch: 6, Step: 214 / 214, losses: 0.0860\n",
      "Epoch: 6 / 10, Epoch loss: 36.82480239868164\n",
      "Epoch: 7, Step: 10 / 214, losses: 0.3264\n",
      "Epoch: 7, Step: 20 / 214, losses: 0.3003\n",
      "Epoch: 7, Step: 30 / 214, losses: 0.2422\n",
      "Epoch: 7, Step: 40 / 214, losses: 0.5887\n",
      "Epoch: 7, Step: 50 / 214, losses: 0.0700\n",
      "Epoch: 7, Step: 60 / 214, losses: 0.0019\n",
      "Epoch: 7, Step: 70 / 214, losses: 0.1551\n",
      "Epoch: 7, Step: 80 / 214, losses: 0.1645\n",
      "Epoch: 7, Step: 90 / 214, losses: 0.0561\n",
      "Epoch: 7, Step: 100 / 214, losses: 0.2936\n",
      "Epoch: 7, Step: 110 / 214, losses: 0.0916\n",
      "Epoch: 7, Step: 120 / 214, losses: 0.1934\n",
      "Epoch: 7, Step: 130 / 214, losses: 0.2613\n",
      "Epoch: 7, Step: 140 / 214, losses: 0.3662\n",
      "Epoch: 7, Step: 150 / 214, losses: 0.0387\n",
      "Epoch: 7, Step: 160 / 214, losses: 0.1440\n",
      "Epoch: 7, Step: 170 / 214, losses: 0.2533\n",
      "Epoch: 7, Step: 180 / 214, losses: 0.0014\n",
      "Epoch: 7, Step: 190 / 214, losses: 0.2176\n",
      "Epoch: 7, Step: 200 / 214, losses: 0.2761\n",
      "Epoch: 7, Step: 210 / 214, losses: 0.2296\n",
      "Epoch: 7, Step: 214 / 214, losses: 0.0811\n",
      "Epoch: 7 / 10, Epoch loss: 34.77053451538086\n",
      "Epoch: 8, Step: 10 / 214, losses: 0.3077\n",
      "Epoch: 8, Step: 20 / 214, losses: 0.2806\n",
      "Epoch: 8, Step: 30 / 214, losses: 0.1999\n",
      "Epoch: 8, Step: 40 / 214, losses: 0.5052\n",
      "Epoch: 8, Step: 50 / 214, losses: 0.0523\n",
      "Epoch: 8, Step: 60 / 214, losses: 0.0032\n",
      "Epoch: 8, Step: 70 / 214, losses: 0.1133\n",
      "Epoch: 8, Step: 80 / 214, losses: 0.1561\n",
      "Epoch: 8, Step: 90 / 214, losses: 0.0485\n",
      "Epoch: 8, Step: 100 / 214, losses: 0.1972\n",
      "Epoch: 8, Step: 110 / 214, losses: 0.0903\n",
      "Epoch: 8, Step: 120 / 214, losses: 0.3613\n",
      "Epoch: 8, Step: 130 / 214, losses: 0.2544\n",
      "Epoch: 8, Step: 140 / 214, losses: 0.5625\n",
      "Epoch: 8, Step: 150 / 214, losses: 0.0520\n",
      "Epoch: 8, Step: 160 / 214, losses: 0.1445\n",
      "Epoch: 8, Step: 170 / 214, losses: 0.2386\n",
      "Epoch: 8, Step: 180 / 214, losses: 0.0007\n",
      "Epoch: 8, Step: 190 / 214, losses: 0.1985\n",
      "Epoch: 8, Step: 200 / 214, losses: 0.3406\n",
      "Epoch: 8, Step: 210 / 214, losses: 0.3415\n",
      "Epoch: 8, Step: 214 / 214, losses: 0.0432\n",
      "Epoch: 8 / 10, Epoch loss: 33.547481536865234\n",
      "Epoch: 9, Step: 10 / 214, losses: 0.4005\n",
      "Epoch: 9, Step: 20 / 214, losses: 0.4209\n",
      "Epoch: 9, Step: 30 / 214, losses: 0.2603\n",
      "Epoch: 9, Step: 40 / 214, losses: 0.4856\n",
      "Epoch: 9, Step: 50 / 214, losses: 0.0198\n",
      "Epoch: 9, Step: 60 / 214, losses: 0.0034\n",
      "Epoch: 9, Step: 70 / 214, losses: 0.0895\n",
      "Epoch: 9, Step: 80 / 214, losses: 0.0886\n",
      "Epoch: 9, Step: 90 / 214, losses: 0.0372\n",
      "Epoch: 9, Step: 100 / 214, losses: 0.1954\n",
      "Epoch: 9, Step: 110 / 214, losses: 0.0612\n",
      "Epoch: 9, Step: 120 / 214, losses: 0.2185\n",
      "Epoch: 9, Step: 130 / 214, losses: 0.1545\n",
      "Epoch: 9, Step: 140 / 214, losses: 0.3624\n",
      "Epoch: 9, Step: 150 / 214, losses: 0.0484\n",
      "Epoch: 9, Step: 160 / 214, losses: 0.1266\n",
      "Epoch: 9, Step: 170 / 214, losses: 0.2902\n",
      "Epoch: 9, Step: 180 / 214, losses: 0.0005\n",
      "Epoch: 9, Step: 190 / 214, losses: 0.1125\n",
      "Epoch: 9, Step: 200 / 214, losses: 0.2703\n",
      "Epoch: 9, Step: 210 / 214, losses: 0.3023\n",
      "Epoch: 9, Step: 214 / 214, losses: 0.0455\n",
      "Epoch: 9 / 10, Epoch loss: 30.502920150756836\n",
      "Epoch: 10, Step: 10 / 214, losses: 0.3886\n",
      "Epoch: 10, Step: 20 / 214, losses: 0.3185\n",
      "Epoch: 10, Step: 30 / 214, losses: 0.2207\n",
      "Epoch: 10, Step: 40 / 214, losses: 0.4093\n",
      "Epoch: 10, Step: 50 / 214, losses: 0.0293\n",
      "Epoch: 10, Step: 60 / 214, losses: 0.0047\n",
      "Epoch: 10, Step: 70 / 214, losses: 0.0895\n",
      "Epoch: 10, Step: 80 / 214, losses: 0.1239\n",
      "Epoch: 10, Step: 90 / 214, losses: 0.0212\n",
      "Epoch: 10, Step: 100 / 214, losses: 0.2771\n",
      "Epoch: 10, Step: 110 / 214, losses: 0.0752\n",
      "Epoch: 10, Step: 120 / 214, losses: 0.3186\n",
      "Epoch: 10, Step: 130 / 214, losses: 0.3050\n",
      "Epoch: 10, Step: 140 / 214, losses: 0.5704\n",
      "Epoch: 10, Step: 150 / 214, losses: 0.0741\n",
      "Epoch: 10, Step: 160 / 214, losses: 0.1979\n",
      "Epoch: 10, Step: 170 / 214, losses: 0.3035\n",
      "Epoch: 10, Step: 180 / 214, losses: 0.0008\n",
      "Epoch: 10, Step: 190 / 214, losses: 0.1359\n",
      "Epoch: 10, Step: 200 / 214, losses: 0.2803\n",
      "Epoch: 10, Step: 210 / 214, losses: 0.3499\n",
      "Epoch: 10, Step: 214 / 214, losses: 0.0941\n",
      "Epoch: 10 / 10, Epoch loss: 32.62116241455078\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.to(device)\n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum = 0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "batch_size = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    i = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k:v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model([imgs[0]], [annotations[0]])\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += losses\n",
    "        \n",
    "        if i%10 == 0 or i == len(data_loader):\n",
    "            print(f\"Epoch: {epoch + 1}, Step: {i} / {int(len_dataloader)}, losses: {losses:.4f}\")\n",
    "    print(f\"Epoch: {epoch + 1} / {num_epochs}, Epoch loss: {epoch_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../model/mask_detection_fasterrcnn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('ml_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57a098aaacedba6950338abd816dea6a7e5d8d49e3ba9c6c8b6b263e04051a9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
